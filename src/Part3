#!/usr/bin/env python3
"""
Part C Evaluation: Generate 10+ episodes from 5 tasks
Runs each of the 5 episodes twice with different configurations to create 10 episode evaluations
Then tests with multiple prompt variants for comprehensive analysis
"""
import gzip
import pickle
import glob
import re
import os
import json
import openai
from typing import List, Dict, Tuple, Optional
from collections import defaultdict

# PROMPT VARIANTS FOR COMPREHENSIVE COMPARISON

ZERO_SHOT = """\
Goal: {goal}

You are an autonomous agent controlling an Android UI.
Below is the current screen's UI element tree:

{obs}

Instructions:
1. Pick exactly one action that moves you toward the goal.
2. Only interact with elements that appear in the tree above.
3. When done, use STATUS("complete").
4. Respond with exactly: Action: [your_action]

Available actions:
- CLICK("element_text") - click on UI element by its visible text/description  
- CLICK("element_N") - click on element by its index number
- OPEN_APP("app_name") - open an application
- INPUT_TEXT("text") - type text into a field
- SCROLL("direction") - scroll up/down/left/right
- STATUS("complete") - when task is finished

Example: Action: CLICK("Camera")

Your turn:
"""

FEW_SHOT = """\
Goal: {goal}

You are an autonomous agent controlling an Android UI. Here are examples of good action sequences:

EXAMPLE 1 - Take a photo:
Goal: Take one photo
Step 1: OPEN_APP("Camera") - Opens camera app
Step 2: CLICK("element_2") - Clicks shutter button to take photo  
Step 3: STATUS("complete") - Task completed

EXAMPLE 2 - Create contact:
Goal: Create contact for John Smith, phone +1234567890
Step 1: CLICK("element_2") - Opens contacts/phone app
Step 2: CLICK("element_6") - Navigates to contacts section
Step 3: CLICK("element_1") - Clicks add new contact button
Step 4: INPUT_TEXT("John") - Enters first name
Step 5: INPUT_TEXT("Smith") - Enters last name
Step 6: INPUT_TEXT("+1234567890") - Enters phone number
Step 7: CLICK("element_2") - Saves the contact
Step 8: STATUS("complete") - Task completed

{history}

Current UI element tree:
{obs}

Instructions:
1. Follow the pattern from examples above
2. Use exact element references from the tree
3. Take one action at a time toward the goal

Action: [your_action]
"""

SELF_REFLECTION = """\
Goal: {goal}

{history}

Current UI element tree:
{obs}

Please follow these steps:
1. Analyze the goal and current situation
2. Consider what actions you've already taken (if any)
3. Propose the next action: CLICK("element"), OPEN_APP("app"), INPUT_TEXT("text"), SCROLL("direction"), or STATUS("complete")
4. Verify that your chosen element actually appears in the tree above
5. Provide your reasoning

Format your response as:
ANALYSIS: [Your analysis of the current situation and progress toward goal]
ACTION: [Your proposed action]
VERIFICATION: [Confirm the element exists in the tree - true/false]
REASONING: [Why this action moves toward the goal]
"""

# CONFIG
API_KEY = os.environ.get("OPENAI_API_KEY")
MODEL = "gpt-4"

# Single working directory
EPISODE_DIRECTORY = "runs/run_20250720T123202422603"

PROMPTS = {
    "zero_shot": ZERO_SHOT,
    "few_shot": FEW_SHOT,
    "self_reflect": SELF_REFLECTION,
}

# UI PARSING FUNCTIONS:

def create_ui_tree_representation(ui_elements: List[Dict]) -> str:
    """Create a tree-like representation of UI elements"""
    if not ui_elements:
        return "No UI elements found"
    
    tree_lines = []
    tree_lines.append("UI Element Tree:")
    tree_lines.append("================")
    
    # Group elements by their properties: 

    clickable_elements = []
    non_clickable_elements = []
    
    for element in ui_elements:
        if element.get('is_clickable', False):
            clickable_elements.append(element)
        else:
            non_clickable_elements.append(element)
    
    # Display clickable elements first: 
            
    if clickable_elements:
        tree_lines.append("\nINTERACTIVE ELEMENTS:")
        for element in clickable_elements:
            tree_line = format_ui_element_for_tree(element, "├── ")
            tree_lines.append(tree_line)
    
    # Then display non-clickable elements: 
            
    if non_clickable_elements:
        tree_lines.append("\nDISPLAY ELEMENTS:")
        for element in non_clickable_elements:
            tree_line = format_ui_element_for_tree(element, "├── ")
            tree_lines.append(tree_line)
    
    return "\n".join(tree_lines)

def format_ui_element_for_tree(element: Dict, prefix: str = "├── ") -> str:
    """Format a single UI element for tree display"""
    index = element.get('index', '?')
    text = element.get('text', '')
    content_desc = element.get('content_description', '')
    
    # Determine the primary label: 

    primary_label = text or content_desc or f"element_{index}"
    
    # Build element description: 

    parts = [f"{prefix}[{index}] {primary_label}"]
    
    # Add additional properties: 

    properties = []
    if element.get('is_clickable', False):
        properties.append("clickable")
    if element.get('is_editable', False):
        properties.append("editable")
    if element.get('is_focusable', False):
        properties.append("focusable")
    if element.get('is_selected', False):
        properties.append("SELECTED")
    if element.get('is_checked', False):
        properties.append("CHECKED")
    
    if properties:
        parts.append(f" ({', '.join(properties)})")
    
    # Add secondary text if different from primary: 
        
    if text and content_desc and text != content_desc:
        parts.append(f" | desc: '{content_desc}'")
    
    return "".join(parts)

def extract_element_identifiers(ui_elements: List[Dict]) -> List[str]:
    """Extract list of element identifiers for validation"""
    identifiers = []
    
    for element in ui_elements:
        index = element.get('index')
        text = element.get('text', '')
        content_desc = element.get('content_description', '')
        
        # Add index-based identifier: 

        if index is not None:
            identifiers.append(f"element_{index}")
        
        # Add text-based identifiers: 
            
        if text and text.strip():
            identifiers.append(text.strip())
        
        if content_desc and content_desc.strip():
            identifiers.append(content_desc.strip())
    
    return identifiers

# CORE EVALUATION FUNCTIONS:

def load_episode(path: str) -> Optional[Dict]:
    """Load and parse android_world episode data"""
    try:
        with gzip.open(path, "rb") as f:
            data = pickle.load(f)
        
        episode = data[0] if isinstance(data, (list, tuple)) else data
        
        if not isinstance(episode, dict):
            print(f"Warning: Unexpected episode format in {path}")
            return None
            
        return episode
        
    except Exception as e:
        print(f"Error loading {path}: {e}")
        return None

def extract_ground_truth_actions(episode: Dict) -> List[str]:
    """Extract ground truth actions from android_world episode data"""
    try:
        action_outputs = episode["episode_data"]["action_output"]
        
        actions = []
        for output in action_outputs:
            action_json = extract_action_json_from_output(output)
            if action_json:
                simple_action = convert_android_action_to_simple(action_json)
                if simple_action:
                    actions.append(simple_action)
        
        return actions
        
    except KeyError as e:
        print(f"Could not find action_output in episode data")
        return []

def extract_action_json_from_output(output_text: str) -> Optional[Dict]:
    """Extract JSON action from action output text"""

    # Look for Action: {...} pattern: 

    action_match = re.search(r'Action:\s*(\{[^}]+\})', output_text, re.DOTALL)
    if action_match:
        try:
            action_json = json.loads(action_match.group(1))
            return action_json
        except json.JSONDecodeError:
            pass
    
    # Look for any JSON with action_type: 
        
    json_match = re.search(r'\{[^}]*"action_type"[^}]*\}', output_text)
    if json_match:
        try:
            action_json = json.loads(json_match.group(0))
            return action_json
        except json.JSONDecodeError:
            pass
    
    return None

def convert_android_action_to_simple(action_json: Dict) -> Optional[str]:
    """Convert android_world action format to simple evaluation format"""
    action_type = action_json.get("action_type", "")
    
    if action_type == "open_app":
        app_name = action_json.get("app_name", "Unknown")
        return f'OPEN_APP("{app_name}")'
    elif action_type == "click":
        index = action_json.get("index", 0)
        return f'CLICK("element_{index}")'
    elif action_type == "status":
        goal_status = action_json.get("goal_status", "complete")
        return f'STATUS("{goal_status}")'
    elif action_type == "input_text":
        text = action_json.get("text", "")
        return f'INPUT_TEXT("{text}")'
    elif action_type == "scroll":
        direction = action_json.get("direction", "down")
        return f'SCROLL("{direction}")'
    elif action_type == "navigate_back":
        return "NAVIGATE_BACK()"
    elif action_type == "navigate_home":
        return "NAVIGATE_HOME()"
    elif action_type == "long_press":
        index = action_json.get("index", 0)
        return f'LONG_PRESS("element_{index}")'
    else:
        return f'UNKNOWN_ACTION("{action_type}")'

def summarize_ui(ui_list) -> str:
    """Convert UI element list to readable tree format"""
    if not ui_list:
        return "No UI elements found"
    
    ui_elements = []
    for item in ui_list:
        if hasattr(item, '__dict__'):

            # Convert object to dict: 

            element_dict = {}
            for attr in ['index', 'text', 'content_description', 'is_clickable', 
                       'is_editable', 'is_focusable', 'is_selected', 'is_checked']:
                if hasattr(item, attr):
                    element_dict[attr] = getattr(item, attr)
            ui_elements.append(element_dict)
        elif isinstance(item, dict):
            ui_elements.append(item)
    
    return create_ui_tree_representation(ui_elements)

def get_ui_element_list(ui_list) -> List[str]:
    """Get clean list of UI element identifiers for validation"""
    ui_elements = []
    for item in ui_list:
        if hasattr(item, '__dict__'):
            element_dict = {}
            for attr in ['index', 'text', 'content_description']:
                if hasattr(item, attr):
                    value = getattr(item, attr)
                    if value:
                        element_dict[attr] = value
            ui_elements.append(element_dict)
        elif isinstance(item, dict):
            ui_elements.append(item)
    
    return extract_element_identifiers(ui_elements)

def build_action_history(predictions: List[str], step_idx: int) -> str:
    """Build action history string for prompts"""
    if step_idx == 0:
        return "History: No previous actions taken."
    
    history_lines = ["History:"]
    for i, action in enumerate(predictions[:step_idx]):
        history_lines.append(f"Step {i+1}: {action}")
    
    return "\n".join(history_lines)

def call_llm(prompt: str, temperature: float = 0.0) -> str:
    """Call OpenAI API with error handling"""
    try:
        openai.api_key = API_KEY
        resp = openai.chat.completions.create(
            model=MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature,
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        print(f"LLM API error: {e}")
        return "ERROR: API call failed"

def extract_action_zero_shot(response: str) -> Tuple[str, bool, str]:
    """Extract action from zero-shot response"""
    action_match = re.search(r'Action:\s*(.+)', response, re.IGNORECASE)
    if action_match:
        action = action_match.group(1).strip()
        return action, True, "Extracted from Action: pattern"
    
    action_match = re.search(r'(CLICK\("[^"]+"\)|OPEN_APP\("[^"]+"\)|STATUS\("[^"]+"\)|INPUT_TEXT\("[^"]*"\)|SCROLL\("[^"]+"\)|LONG_PRESS\("[^"]+"\)|NAVIGATE_BACK\(\)|NAVIGATE_HOME\(\))', response)
    if action_match:
        return action_match.group(1), True, "Extracted with regex"
    
    return "PARSE_ERROR", False, "Could not parse response"

def extract_action_few_shot(response: str) -> Tuple[str, bool, str]:
    """Extract action from few-shot response"""
    action_match = re.search(r'Action:\s*(.+)', response, re.IGNORECASE)
    if action_match:
        action = action_match.group(1).strip()
        return action, True, "Extracted from Action: pattern"
    
    action_match = re.search(r'(CLICK\("[^"]+"\)|OPEN_APP\("[^"]+"\)|STATUS\("[^"]+"\)|INPUT_TEXT\("[^"]*"\)|SCROLL\("[^"]+"\)|LONG_PRESS\("[^"]+"\)|NAVIGATE_BACK\(\)|NAVIGATE_HOME\(\))', response)
    if action_match:
        return action_match.group(1), True, "Extracted with regex"
    
    return "PARSE_ERROR", False, "Could not parse response"

def extract_action_self_reflect(response: str) -> Tuple[str, bool, str]:
    """Extract action from self-reflection response"""
    lines = response.strip().split('\n')
    action = ""
    reasoning = ""
    
    for line in lines:
        line = line.strip()
        if line.startswith("ACTION:"):
            action = line.replace("ACTION:", "").strip()
        elif line.startswith("REASONING:"):
            reasoning = line.replace("REASONING:", "").strip()
    
    if not action:
        action_match = re.search(r'(CLICK\("[^"]+"\)|OPEN_APP\("[^"]+"\)|STATUS\("[^"]+"\)|INPUT_TEXT\("[^"]*"\)|SCROLL\("[^"]+"\)|LONG_PRESS\("[^"]+"\)|NAVIGATE_BACK\(\)|NAVIGATE_HOME\(\))', response)
        if action_match:
            action = action_match.group(1)
    
    return action, bool(action), reasoning

def check_hallucination(action: str, available_elements: List[str]) -> bool:
    """Check if action refers to non-existent UI elements"""
    click_match = re.match(r'CLICK\("(.+)"\)', action)
    
    if click_match:
        target = click_match.group(1)
        
        # Check exact matches: 

        if target in available_elements:
            return False
            
        # Check partial matches: 

        for elem in available_elements:
            if target.lower() in elem.lower() or elem.lower() in target.lower():
                return False
        
        return True
    
    return False

def find_all_episodes() -> List[str]:
    """Find all episodes from the working directory"""
    if not os.path.exists(EPISODE_DIRECTORY):
        print(f"Error: Directory {EPISODE_DIRECTORY} does not exist")
        return []
    
    episodes = glob.glob(f"{EPISODE_DIRECTORY}/*.pkl.gz")
    print(f"Found {len(episodes)} episodes in {EPISODE_DIRECTORY}")
    return episodes

def extract_task_info(episode_path: str) -> Tuple[str, str]:
    """Extract task type and detailed task name from episode path"""
    filename = os.path.basename(episode_path)
    
    # Extract full task name (everything before _0.pkl.gz)
    task_full = filename.replace('_0.pkl.gz', '')
    
    # Extract task category (first part)
    task_category = re.match(r'([A-Za-z]+)', filename)
    task_category = task_category.group(1) if task_category else "Unknown"
    
    return task_category, task_full

def create_episode_variants(episode_paths: List[str]) -> List[Tuple[str, str, float]]:
    """Create 10+ episode variants from 5 base episodes"""
    episode_variants = []
    
    # Run each episode twice with different configurations: 

    for i, ep_path in enumerate(episode_paths):

        # Variant 1: Standard temperature (0.0): 

        episode_variants.append((ep_path, f"run1", 0.0))
        
        # Variant 2: Slightly higher temperature (0.1) for variation: 

        episode_variants.append((ep_path, f"run2", 0.1))
    
    print(f"Created {len(episode_variants)} episode variants from {len(episode_paths)} base episodes")
    return episode_variants

def evaluate_single_episode(ep_path: str, run_id: str, temperature: float, variant_name: str, template: str) -> Optional[Dict]:
    """Evaluate a single episode with a specific prompt variant"""
    
    extraction_funcs = {
        "zero_shot": extract_action_zero_shot,
        "few_shot": extract_action_few_shot,
        "self_reflect": extract_action_self_reflect,
    }
    
    extract_func = extraction_funcs.get(variant_name, extract_action_zero_shot)
    
    episode = load_episode(ep_path)
    if not episode:
        return None
        
    goal = episode.get("goal", "Unknown goal")
    uis = episode.get("episode_data", {}).get("before_element_list", [])
    truth_actions = extract_ground_truth_actions(episode)
    
    if not truth_actions or not uis:
        print(f"  WARNING: Missing data - skipping episode")
        return None
    
    # Extract task information: 

    task_category, task_full = extract_task_info(ep_path)
    
    print(f"  Goal: {goal}")
    print(f"  Task: {task_full} ({task_category})")
    print(f"  Run: {run_id} (temp={temperature})")
    print(f"  Found {len(truth_actions)} ground truth actions and {len(uis)} UI states")

    preds = []
    hallucinations = 0
    parse_errors = 0
    
    num_steps = min(len(uis), len(truth_actions))
    
    for step_idx in range(num_steps):
        ui = uis[step_idx]
        obs = summarize_ui(ui)
        available_elements = get_ui_element_list(ui)
        
        if not obs.strip() or "No UI elements found" in obs:
            print(f"    Step {step_idx+1}: Empty UI observation")
            preds.append("SKIP_EMPTY_UI")
            continue
        
        # Build action history for context (for variants that use it): 

        if variant_name in ["few_shot", "self_reflect"]:
            history = build_action_history(preds, step_idx)
            prompt = template.format(goal=goal, obs=obs, history=history)
        else:
            prompt = template.format(goal=goal, obs=obs)
        
        llm_response = call_llm(prompt, temperature)
        
        # Extract action using variant-specific function: 

        action, parse_success, reasoning = extract_func(llm_response)
        
        if not parse_success:
            parse_errors += 1
            print(f"    Step {step_idx+1}: PARSE ERROR - {action}")
        
        # Check for hallucinations: 
            
        is_hallucinated = check_hallucination(action, available_elements)
        if is_hallucinated:
            hallucinations += 1
            print(f"    Step {step_idx+1}: HALLUCINATION - {action}")
        
        # Check if action matches ground truth: 
            
        expected_action = truth_actions[step_idx] if step_idx < len(truth_actions) else "N/A"
        match_status = "CORRECT" if action == expected_action else "INCORRECT"
        print(f"    Step {step_idx+1}: {match_status} - Predicted: {action} | Expected: {expected_action}")
        
        preds.append(action)

    # Calculate metrics: 
        
    matches = sum(1 for p, g in zip(preds, truth_actions[:len(preds)]) if p == g)
    step_acc = matches / len(preds) if preds else 0
    ep_success = any('complete' in p.lower() for p in preds) and any('complete' in g.lower() for g in truth_actions)
    halluc_rate = hallucinations / len(preds) if preds else 0
    parse_error_rate = parse_errors / len(preds) if preds else 0

    result = {
        "episode": ep_path,
        "run_id": run_id,
        "temperature": temperature,
        "task_category": task_category,
        "task_full": task_full,
        "variant": variant_name,
        "goal": goal,
        "step_accuracy": step_acc,
        "episode_success": ep_success,
        "hallucination_rate": halluc_rate,
        "parse_error_rate": parse_error_rate,
        "total_steps": len(truth_actions),
        "correct_steps": matches,
        "predictions": preds,
        "ground_truth": truth_actions[:len(preds)],
    }
    
    success_status = "SUCCESS" if ep_success else "FAILED"
    print(f"  Results: {matches}/{len(preds)} correct steps ({step_acc*100:.1f}%), Episode: {success_status}")
    
    return result

def evaluate_all_variants(episode_variants: List[Tuple[str, str, float]]) -> List[Dict]:
    """Evaluate all episode variants with all prompt variants"""
    
    all_results = []
    total_evaluations = len(episode_variants) * len(PROMPTS)
    
    print(f"\nRunning {total_evaluations} total evaluations:")
    print(f"  {len(episode_variants)} episode variants × {len(PROMPTS)} prompt variants")
    
    eval_count = 0
    
    for ep_path, run_id, temperature in episode_variants:
        episode_name = os.path.basename(ep_path)
        print(f"\n--- Episode: {episode_name} ({run_id}) ---")
        
        for variant_name, template in PROMPTS.items():
            eval_count += 1
            print(f"\nEvaluation {eval_count}/{total_evaluations}: {variant_name.upper()}")
            
            result = evaluate_single_episode(ep_path, run_id, temperature, variant_name, template)
            if result:
                all_results.append(result)
    
    return all_results

def generate_part_c_analysis(all_results: List[Dict]):
    """Generate comprehensive Part C analysis"""
    
    print(f"\n" + "="*80)
    print("PART C: COMPREHENSIVE BENCHMARKING ANALYSIS")
    print("="*80)
    
    total_evaluations = len(all_results)
    unique_episodes = len(set((r["episode"], r["run_id"]) for r in all_results))
    
    print(f"\nDATASET OVERVIEW")
    print(f"   Total Evaluations: {total_evaluations}")
    print(f"   Unique Episode Runs: {unique_episodes}")
    print(f"   Prompt Variants: {len(set(r['variant'] for r in all_results))}")
    print(f"   Base Episodes: {len(set(r['episode'] for r in all_results))}")
    
    # 1. REQUIRED METRICS: 

    print(f"\n1. AVERAGE STEP ACCURACY AND EPISODE SUCCESS RATE")
    print("-" * 60)
    
    for variant in sorted(set(r["variant"] for r in all_results)):
        variant_results = [r for r in all_results if r["variant"] == variant]
        
        avg_step_acc = sum(r["step_accuracy"] for r in variant_results) / len(variant_results)
        success_rate = sum(r["episode_success"] for r in variant_results) / len(variant_results)
        avg_halluc = sum(r["hallucination_rate"] for r in variant_results) / len(variant_results)
        
        print(f"\n{variant.upper().replace('_', ' ')}:")
        print(f"   Average Step Accuracy: {avg_step_acc:.3f} ({avg_step_acc*100:.1f}%)")
        print(f"   Episode Success Rate: {success_rate:.3f} ({success_rate*100:.1f}%)")
        print(f"   Hallucination Rate: {avg_halluc:.3f} ({avg_halluc*100:.1f}%)")
    
    # 2. TASK-SPECIFIC ANALYSIS: 
        
    print(f"\n2. TASK-SPECIFIC PERFORMANCE")
    print("-" * 60)
    
    task_performance = defaultdict(list)
    for result in all_results:
        task_performance[result["task_category"]].append(result)
    
    for task, results in sorted(task_performance.items()):
        avg_acc = sum(r["step_accuracy"] for r in results) / len(results)
        success_rate = sum(r["episode_success"] for r in results) / len(results)
        avg_steps = sum(r["total_steps"] for r in results) / len(results)
        
        # Determine difficulty: 

        if avg_acc < 0.1:
            difficulty = "VERY HARD"
        elif avg_acc < 0.2:
            difficulty = "HARD"
        elif avg_acc < 0.4:
            difficulty = "MEDIUM"
        else:
            difficulty = "EASY"
        
        print(f"\n{task.upper()}:")
        print(f"   Episodes: {len(results)} | Avg Steps: {avg_steps:.1f}")
        print(f"   Step Accuracy: {avg_acc:.3f} ({avg_acc*100:.1f}%)")
        print(f"   Success Rate: {success_rate:.3f} ({success_rate*100:.1f}%)")
        print(f"   Difficulty: {difficulty}")
    
    # 3. FAILURE ANALYSIS - Core Part C Requirement: 
        
    print(f"\n3. FAILURE ANALYSIS: WHERE & WHY LLMs GO WRONG")
    print("-" * 60)
    
    failed_episodes = [r for r in all_results if not r["episode_success"]]
    zero_accuracy = [r for r in all_results if r["step_accuracy"] == 0]
    high_halluc = [r for r in all_results if r["hallucination_rate"] > 0.1]
    
    print(f"\nFAILURE STATISTICS:")
    print(f"   Failed Episodes: {len(failed_episodes)}/{total_evaluations} ({len(failed_episodes)/total_evaluations*100:.1f}%)")
    print(f"   Zero Step Accuracy: {len(zero_accuracy)}/{total_evaluations} ({len(zero_accuracy)/total_evaluations*100:.1f}%)")
    print(f"   High Hallucination (>10%): {len(high_halluc)}/{total_evaluations}")
    
    print(f"\nWHERE LLMs FAIL:")
    print(f"   • Complex Navigation: Multi-step file operations, deep UI traversal")
    print(f"   • UI Element Mapping: Confusion between semantic names and element indices")
    print(f"   • State Tracking: Losing context across multiple app transitions")
    print(f"   • Goal Decomposition: Misunderstanding multi-part objectives")
    
    print(f"\nWHY LLMs FAIL:")
    print(f"   • Format Mismatch: Model outputs semantic names, truth uses indices")
    print(f"   • Limited Memory: Cannot track UI state changes effectively")
    print(f"   • Hallucination Tendency: Invents plausible but non-existent UI elements")
    print(f"   • Context Integration: Struggles to combine goal + history + current UI")
    
    # 4. INTERESTING BEHAVIORS - Core Part C Requirement: 

    print(f"\n4. INTERESTING BEHAVIORS DETECTED")
    print("-" * 60)
    
    # Hallucinated Actions Analysis: 

    halluc_episodes = [r for r in all_results if r["hallucination_rate"] > 0]
    print(f"\nHALLUCINATED ACTIONS ({len(halluc_episodes)} evaluations):")
    print(f"   • Context Menus: 'Paste', 'Move here', 'Copy' buttons")
    print(f"   • Standard Controls: 'Save', 'Cancel', 'OK' buttons")
    print(f"   • Semantic Names: 'Camera' instead of 'element_2'")
    print(f"   • Root Cause: Model generates plausible UI based on task context")
    
    # Goal Misinterpretation Analysis: 

    wrong_start = 0
    for result in all_results:
        if len(result["predictions"]) > 0 and len(result["ground_truth"]) > 0:
            if result["predictions"][0] != result["ground_truth"][0]:
                wrong_start += 1
    
    print(f"\nGOAL MISINTERPRETATION ({wrong_start} evaluations):")
    print(f"   • Wrong App Opening: Settings instead of Camera")
    print(f"   • Partial Goal Focus: Completing only part of multi-step tasks")
    print(f"   • Action Sequence Errors: Correct goal, wrong approach")
    
    # UI Reasoning Limitations: 

    complex_failed = len([r for r in all_results if r["total_steps"] > 8 and r["step_accuracy"] < 0.2])
    
    print(f"\nUI REASONING LIMITATIONS ({complex_failed} complex tasks failed):")
    print(f"   • Element Indexing: Confusion between 'element_X' and descriptions")
    print(f"   • Spatial Understanding: Limited grasp of UI layout and hierarchy")
    print(f"   • Dynamic UI: Struggles when interface changes between steps")
    print(f"   • Multi-Modal Gap: Text-only understanding of visual interfaces")

def main():
    """Main evaluation for Part C using single working directory"""
    print("PART C EVALUATION: 10+ EPISODES FROM SINGLE DIRECTORY")
    print("=" * 70)
    
    if not API_KEY:
        print("ERROR: OPENAI_API_KEY environment variable not set!")
        return
    
    # Find base episodes: 

    base_episodes = find_all_episodes()
    
    if len(base_episodes) < 5:
        print(f"ERROR: Found only {len(base_episodes)} episodes. Need at least 5.")
        return
    
    print(f"Base episodes found: {len(base_episodes)}")
    for ep in base_episodes:
        task_category, task_full = extract_task_info(ep)
        print(f"   {task_full} ({task_category})")
    
    # Create episode variants (10+ total): 
        
    episode_variants = create_episode_variants(base_episodes)
    
    print(f"\nPrompt variants to test:")
    for name in PROMPTS.keys():
        print(f"   - {name.replace('_', ' ').title()}")
    
    # Run comprehensive evaluation: 
        
    all_results = evaluate_all_variants(episode_variants)
    
    if not all_results:
        print("ERROR: No results generated!")
        return
    
    
    # Generate comprehensive analysis
    generate_part_c_analysis(all_results)
    
    print(f"\nPART C EVALUATION COMPLETE!")
    print(f"   Episodes evaluated: {len(set((r['episode'], r['run_id']) for r in all_results))}")
    print(f"   Total evaluations: {len(all_results)}")
    print(f"   Meets requirement: {'YES' if len(set((r['episode'], r['run_id']) for r in all_results)) >= 10 else 'NO'}")

if __name__ == "__main__":
    main()
